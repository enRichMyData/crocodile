{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables...:   0%|          | 0/7207 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    ne_cols = {str(col_index) for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{int(row[1])-1}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset = \"HardTablesR3\"\n",
    "    cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "    tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "    for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "        if table.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "            table_name = table.split(\".csv\")[0]\n",
    "            ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "            break    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0'},\n",
       " {'0-0': 'Q1084490',\n",
       "  '1-0': 'Q1046685',\n",
       "  '2-0': 'Q957787',\n",
       "  '3-0': 'Q882739',\n",
       "  '4-0': 'Q879904'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_cols, correct_qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    ne_cols = {str(col_index) for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{int(row[1])-1}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint\n",
    "def classify_columns_with_api(df, type=\"accurate\"):\n",
    "    payload = {\n",
    "        \"json\": [df.T.astype(str).values.tolist()]  # Transpose to align columns and convert to list of rows\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "for dataset in datasets:\n",
    "    cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "    tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "    \n",
    "    for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "        if table.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "            table_name = table.split(\".csv\")[0]\n",
    "\n",
    "            # Get NE columns and correct QIDs from GT\n",
    "            ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "            \n",
    "            # Call the NER API to classify columns\n",
    "            ner_response = classify_columns_with_api(df)\n",
    "            if ner_response:\n",
    "                lit_cols = {}  # Initialize LIT columns dictionary\n",
    "                for col_idx, ner_info in ner_response.items():\n",
    "                    print(ner_info)\n",
    "                    col_num = str(ner_info[\"index_column\"])  # Get column index as string\n",
    "                    classification = ner_info[\"classification\"]  # Get NER or LIT type\n",
    "                    tag = ner_info[\"tag\"]\n",
    "\n",
    "                    # Update NE columns from GT\n",
    "                    if col_num in ne_cols:\n",
    "                        # If GT classifies as NE, we trust the GT and update with the NER type\n",
    "                        if tag == \"LIT\":\n",
    "                            classification = \"OTHER\"\n",
    "                        ne_cols[col_num] = classification\n",
    "                    elif tag == \"NE\":\n",
    "                        # If API classifies as NE and not in GT, add to ne_cols\n",
    "                        ne_cols[col_num] = classification\n",
    "                    elif tag == \"LIT\":\n",
    "                        # If API classifies as LIT, add to lit_cols\n",
    "                        lit_cols[col_num] = classification\n",
    "        break\n",
    "    break  # Keep the break for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table_1': {'0': {'index_column': 0,\n",
       "    'tag': 'LIT',\n",
       "    'classification': 'NUMBER',\n",
       "    'datatype': 'NUMBER',\n",
       "    'probabilities': {'NUMBER': 1.0}},\n",
       "   '1': {'index_column': 1,\n",
       "    'tag': 'NE',\n",
       "    'classification': 'OTHER',\n",
       "    'datatype': 'OTHER',\n",
       "    'probabilities': {'LOCATION': 0.06,\n",
       "     'PERSON': 0.02,\n",
       "     'OTHER': 0.84,\n",
       "     'STRING': 0.06}},\n",
       "   '2': {'index_column': 2,\n",
       "    'tag': 'LIT',\n",
       "    'classification': 'NUMBER',\n",
       "    'datatype': 'NUMBER',\n",
       "    'probabilities': {'NUMBER': 1.0, 'DATE': 1.0}},\n",
       "   '3': {'index_column': 3,\n",
       "    'tag': 'NE',\n",
       "    'classification': 'PERSON',\n",
       "    'datatype': 'PERSON',\n",
       "    'probabilities': {'PERSON': 1.0}},\n",
       "   '4': {'index_column': 4,\n",
       "    'tag': 'LIT',\n",
       "    'classification': 'NUMBER',\n",
       "    'datatype': 'NUMBER',\n",
       "    'probabilities': {'NUMBER': 1.0}}}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ne_cols, lit_cols, \u001b[43mtag\u001b[49m, ner_response\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tag' is not defined"
     ]
    }
   ],
   "source": [
    "ne_cols, lit_cols, tag, ner_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Somosierra</td>\n",
       "      <td>Autovía A-1</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fotu La</td>\n",
       "      <td>National Highway 1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zojila Pass</td>\n",
       "      <td>National Highway 1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jablunkov Pass</td>\n",
       "      <td>European route E75</td>\n",
       "      <td>Czech Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wildhaus Pass</td>\n",
       "      <td>Main road 16</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pacific Grade Summit</td>\n",
       "      <td>California State Route 4</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mahoosuc Notch</td>\n",
       "      <td>Appalachian Trail</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zealand Notch</td>\n",
       "      <td>Appalachian Trail</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summit Pass</td>\n",
       "      <td>Alaska Highway</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Muncho Pass</td>\n",
       "      <td>Alaska Highway</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pine Pass</td>\n",
       "      <td>Canadian National Railway</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dyulino Pass</td>\n",
       "      <td>Cherno More motorway</td>\n",
       "      <td>Bulgaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Vitinya Pass</td>\n",
       "      <td>Hemus motorway</td>\n",
       "      <td>Bulgaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Gate of Trajan</td>\n",
       "      <td>Trakia motorway</td>\n",
       "      <td>Bulgaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Postojna Gate</td>\n",
       "      <td>A1 motorway</td>\n",
       "      <td>Slovenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Packsattel</td>\n",
       "      <td>Süd Autobahn</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Satus Pass</td>\n",
       "      <td>U.S. Route 97</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Blewett Pass</td>\n",
       "      <td>U.S. Route 97</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kingman Pass</td>\n",
       "      <td>U.S. Route 89</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Vulcan Pass</td>\n",
       "      <td>Jiu River</td>\n",
       "      <td>Romania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    col0                       col1                      col2\n",
       "0             Somosierra                Autovía A-1                     Spain\n",
       "1                Fotu La         National Highway 1                     India\n",
       "2            Zojila Pass         National Highway 1                     India\n",
       "3         Jablunkov Pass         European route E75            Czech Republic\n",
       "4          Wildhaus Pass               Main road 16               Switzerland\n",
       "5   Pacific Grade Summit   California State Route 4  United States of America\n",
       "6         Mahoosuc Notch          Appalachian Trail  United States of America\n",
       "7          Zealand Notch          Appalachian Trail  United States of America\n",
       "8            Summit Pass             Alaska Highway                    Canada\n",
       "9            Muncho Pass             Alaska Highway                    Canada\n",
       "10             Pine Pass  Canadian National Railway                    Canada\n",
       "11          Dyulino Pass       Cherno More motorway                  Bulgaria\n",
       "12          Vitinya Pass             Hemus motorway                  Bulgaria\n",
       "13        Gate of Trajan            Trakia motorway                  Bulgaria\n",
       "14         Postojna Gate                A1 motorway                  Slovenia\n",
       "15            Packsattel               Süd Autobahn                   Austria\n",
       "16            Satus Pass              U.S. Route 97  United States of America\n",
       "17          Blewett Pass              U.S. Route 97  United States of America\n",
       "18          Kingman Pass              U.S. Route 89  United States of America\n",
       "19           Vulcan Pass                  Jiu River                   Romania"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    # Initialize ne_cols as a dict where the value (NER type) is None initially\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{int(row[1])-1}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint\n",
    "def classify_columns_with_api(df, type=\"accurate\"):\n",
    "    payload = {\n",
    "        \"json\": df.T.astype(str).values.tolist()  # Transpose to align columns and convert to list of rows\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[0][\"table_1\"]  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to onboard data into MongoDB\n",
    "def onboard_data(dataset_name, table_name, df, ne_cols, correct_qids, ner_response):\n",
    "    lit_cols = {}\n",
    "    all_columns = set([str(i) for i in range(len(df.columns))])  # Set of all columns (as strings)\n",
    "\n",
    "    # Parse the API response for NE and LIT columns\n",
    "    if ner_response:\n",
    "        for col_idx, ner_info in ner_response.items():\n",
    "            col_num = str(ner_info[\"index_column\"])\n",
    "            classification = ner_info[\"classification\"]\n",
    "            tag = ner_info[\"tag\"]\n",
    "            \n",
    "            if col_num in ne_cols:\n",
    "                # If GT classifies as NE, we trust the GT and update with NER type\n",
    "                if tag == \"LIT\":\n",
    "                    classification = \"OTHER\"\n",
    "                ne_cols[col_num] = classification\n",
    "            elif tag == \"LIT\":\n",
    "                # If API classifies as LIT, add to LIT columns\n",
    "                lit_cols[col_num] = classification\n",
    "\n",
    "    # Compute unclassified columns by subtracting NE and LIT columns from all columns\n",
    "    classified_ne_columns = set(ne_cols.keys())\n",
    "    classified_lit_columns = set(lit_cols.keys())\n",
    "    classified_columns = classified_ne_columns.union(classified_lit_columns)\n",
    "    unclassified_columns = all_columns.difference(classified_columns)\n",
    "    \n",
    "    # Insert data row by row\n",
    "    for index, row in df.iterrows():\n",
    "        document = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": index,\n",
    "            \"data\": row.to_dict(),\n",
    "            \"classified_columns\": {\n",
    "                \"NE\": ne_cols,  # Use updated NE columns\n",
    "                \"LIT\": lit_cols,  # Use LIT columns from the API response\n",
    "                \"UNCLASSIFIED\": list(unclassified_columns)  # Unclassified columns\n",
    "            },\n",
    "            \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns\n",
    "            \"correct_qids\": correct_qids,  # Correct QIDs from GT\n",
    "            \"status\": \"TODO\"\n",
    "        }\n",
    "\n",
    "        # Insert or update the document in MongoDB\n",
    "        input_collection.update_one(\n",
    "            {\"dataset_name\": dataset_name, \"table_name\": table_name, \"row_id\": index},\n",
    "            {\"$set\": document},\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "    # Log onboarding completion for table-level trace\n",
    "    table_trace_collection.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"total_rows\": len(df),\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    # Add table to process queue\n",
    "    process_queue.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"status\": \"QUEUED\",\n",
    "            \"table_name\": table_name,\n",
    "            \"dataset_name\": dataset_name\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "# Main processing loop for onboarding datasets\n",
    "for dataset in datasets:\n",
    "    cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "    tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "    \n",
    "    for table in tqdm(tables, desc=f\"Processing tables for dataset {dataset}...\"):\n",
    "        if table.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "            table_name = table.split(\".csv\")[0]\n",
    "\n",
    "            # Get NE columns and correct QIDs from GT\n",
    "            ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "            \n",
    "            # Call the NER API to classify columns\n",
    "            ner_response = classify_columns_with_api(df, type=\"fast\")\n",
    "\n",
    "            # Onboard the data into MongoDB\n",
    "            onboard_data(dataset, table_name, df, ne_cols, correct_qids, ner_response)\n",
    "\n",
    "    # Initialize dataset-level trace after onboarding all tables in the dataset\n",
    "    dataset_trace_collection.update_one(\n",
    "        {\"dataset_name\": dataset},\n",
    "        {\"$setOnInsert\": {\n",
    "            \"total_tables\": len(tables),\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": 0,  # Updated as tables are processed\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables...: 100%|██████████| 64/64 [00:07<00:00,  8.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': 'NE'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    ne_cols = {str(col_index): \"NE\" for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{int(row[1])-1}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint with multiple tables\n",
    "def classify_columns_with_api(tables_data, type=\"fast\"):\n",
    "    payload = {\n",
    "        \"json\": [df.T.astype(str).values.tolist() for df in tables_data]  # Multiple tables as input\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "def process_tables(datasets, max_tables_at_once=1):\n",
    "    for dataset in datasets:\n",
    "        cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "        tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "        tables_data = []\n",
    "        processed_count = 0\n",
    "\n",
    "        for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "            if table.endswith(\".csv\"):\n",
    "                df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "                table_name = table.split(\".csv\")[0]\n",
    "\n",
    "                # Get NE columns and correct QIDs from GT\n",
    "                ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "                \n",
    "                # Add table data to list\n",
    "                tables_data.append(df)\n",
    "                processed_count += 1\n",
    "\n",
    "                # If we reach the maximum number of tables, make the API call\n",
    "                if processed_count >= max_tables_at_once:\n",
    "                    # Call the NER API to classify columns for these tables\n",
    "                    ner_response = classify_columns_with_api(tables_data)\n",
    "                    \n",
    "                    if ner_response:\n",
    "                        for table_response in ner_response:\n",
    "                            table_key = list(table_response.keys())[0]\n",
    "                            lit_cols = {}  # Initialize LIT columns dictionary\n",
    "                            for col_idx, ner_info in table_response[table_key].items():\n",
    "                                col_num = str(ner_info[\"index_column\"])  # Get column index as string\n",
    "                                classification = ner_info[\"classification\"]  # Get NER or LIT type\n",
    "                                tag = ner_info[\"tag\"]\n",
    "\n",
    "                                # Update NE columns from GT\n",
    "                                if col_num in ne_cols:\n",
    "                                    # If GT classifies as NE, we trust the GT and update with the NER type\n",
    "                                    if tag == \"LIT\":\n",
    "                                        classification = \"OTHER\"\n",
    "                                    ne_cols[col_num] = classification\n",
    "                                elif tag == \"LIT\":\n",
    "                                    # If API classifies as LIT, add to lit_cols\n",
    "                                    lit_cols[col_num] = classification\n",
    "\n",
    "                    # Reset tables data for the next batch\n",
    "                    tables_data = []\n",
    "                    processed_count = 0\n",
    "            \n",
    "        # In case there are remaining tables that haven't been processed yet\n",
    "        if tables_data:\n",
    "            ner_response = classify_columns_with_api(tables_data)\n",
    "        return ne_cols\n",
    "\n",
    "# Example of running the function\n",
    "process_tables(datasets, max_tables_at_once=10)  # Adjust the number of tables you want to submit at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables...: 100%|██████████| 64/64 [00:07<00:00,  8.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    # Initialize ne_cols as a dict where the value (NER type) is None initially\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{row[1]}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint with multiple tables\n",
    "def classify_columns_with_api(tables_data, type=\"fast\"):\n",
    "    payload = {\n",
    "        \"json\": [df.T.astype(str).values.tolist() for df in tables_data]  # Multiple tables as input\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "def process_tables(datasets, max_tables_at_once=1):\n",
    "    for dataset in datasets:\n",
    "        cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "        tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "        tables_data = []\n",
    "        processed_count = 0\n",
    "\n",
    "        for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "            if table.endswith(\".csv\"):\n",
    "                df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "                table_name = table.split(\".csv\")[0]\n",
    "\n",
    "                # Get NE columns and correct QIDs from GT\n",
    "                ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "                \n",
    "                # Add table data to list\n",
    "                tables_data.append(df)\n",
    "                processed_count += 1\n",
    "\n",
    "                # If we reach the maximum number of tables, make the API call\n",
    "                if processed_count >= max_tables_at_once:\n",
    "                    # Call the NER API to classify columns for these tables\n",
    "                    ner_response = classify_columns_with_api(tables_data)\n",
    "                    \n",
    "                    if ner_response:\n",
    "                        for table_response in ner_response:\n",
    "                            table_key = list(table_response.keys())[0]\n",
    "                            lit_cols = {}  # Initialize LIT columns dictionary\n",
    "                            for col_idx, ner_info in table_response[table_key].items():\n",
    "                                col_num = str(ner_info[\"index_column\"])  # Get column index as string\n",
    "                                classification = ner_info[\"classification\"]  # Get NER or LIT type\n",
    "                                tag = ner_info[\"tag\"]\n",
    "\n",
    "                                # Update NE columns from GT\n",
    "                                if col_num in ne_cols:\n",
    "                                    # If GT classifies as NE, we trust the GT and update with the NER type\n",
    "                                    if tag == \"LIT\":\n",
    "                                        classification = \"OTHER\"\n",
    "                                    ne_cols[col_num] = classification\n",
    "                                elif tag == \"NE\":\n",
    "                                    # If API classifies as NE and not in GT, add to ne_cols\n",
    "                                    ne_cols[col_num] = classification\n",
    "                                elif tag == \"LIT\":\n",
    "                                    # If API classifies as LIT, add to lit_cols\n",
    "                                    lit_cols[col_num] = classification\n",
    "\n",
    "                    # Reset tables data for the next batch\n",
    "                    tables_data = []\n",
    "                    processed_count = 0\n",
    "        \n",
    "        # In case there are remaining tables that haven't been processed yet\n",
    "        if tables_data:\n",
    "            ner_response = classify_columns_with_api(tables_data)\n",
    "        return\n",
    "\n",
    "# Example of running the function\n",
    "process_tables(datasets, max_tables_at_once=10)  # Adjust the number of tables you want to submit at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3_2019\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    # Initialize ne_cols as a dict where the value (NER type) is None initially\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{int(row[1])-1}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API with multiple tables at once and random sample of 100 rows\n",
    "def classify_columns_with_api(tables_data, type=\"fast\", sample_size=100):\n",
    "    # For each table, take a random sample of 100 rows if the table has more than 100 rows\n",
    "    sampled_tables = [df.sample(n=min(sample_size, len(df)), random_state=42) for df in tables_data]\n",
    "\n",
    "    payload = {\n",
    "        \"json\": [df.T.astype(str).values.tolist() for df in sampled_tables]  # Multiple tables as input\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Increase the timeout to accommodate longer processing times\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=300)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to onboard data into MongoDB\n",
    "def onboard_data(dataset_name, table_name, df, ne_cols, correct_qids, ner_response):\n",
    "    lit_cols = {}\n",
    "    all_columns = set([str(i) for i in range(len(df.columns))])  # Set of all columns (as strings)\n",
    "    \n",
    "    # Parse the API response for NE and LIT columns\n",
    "    if ner_response:\n",
    "        for col_idx, ner_info in ner_response.items():\n",
    "            col_num = str(ner_info[\"index_column\"])\n",
    "            classification = ner_info[\"classification\"]\n",
    "            tag = ner_info[\"tag\"]\n",
    "            \n",
    "            if col_num in ne_cols:\n",
    "                # If GT classifies as NE, we trust the GT and update with NER type\n",
    "                if tag == \"LIT\":\n",
    "                    classification = \"OTHER\"\n",
    "                ne_cols[col_num] = classification\n",
    "            elif tag == \"LIT\":\n",
    "                # If API classifies as LIT, add to LIT columns\n",
    "                lit_cols[col_num] = classification\n",
    "\n",
    "    # Compute unclassified columns by subtracting NE and LIT columns from all columns\n",
    "    classified_ne_columns = set(ne_cols.keys())\n",
    "    classified_lit_columns = set(lit_cols.keys())\n",
    "    classified_columns = classified_ne_columns.union(classified_lit_columns)\n",
    "    unclassified_columns = all_columns.difference(classified_columns)\n",
    "    \n",
    "    # Insert data row by row\n",
    "    for index, row in df.iterrows():\n",
    "        document = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": index,\n",
    "            \"data\": row.to_dict(),\n",
    "            \"classified_columns\": {\n",
    "                \"NE\": ne_cols,  # Use updated NE columns\n",
    "                \"LIT\": lit_cols,  # Use LIT columns from the API response\n",
    "                \"UNCLASSIFIED\": list(unclassified_columns)  # Unclassified columns\n",
    "            },\n",
    "            \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns\n",
    "            \"correct_qids\": correct_qids,  # Correct QIDs from GT\n",
    "            \"status\": \"TODO\"\n",
    "        }\n",
    "\n",
    "        # Insert or update the document in MongoDB\n",
    "        input_collection.update_one(\n",
    "            {\"dataset_name\": dataset_name, \"table_name\": table_name, \"row_id\": index},\n",
    "            {\"$set\": document},\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "    # Log onboarding completion for table-level trace\n",
    "    table_trace_collection.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"total_rows\": len(df),\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    # Add table to process queue\n",
    "    process_queue.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"status\": \"QUEUED\",\n",
    "            \"table_name\": table_name,\n",
    "            \"dataset_name\": dataset_name\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "# Main processing loop for onboarding datasets\n",
    "def process_tables(datasets, max_tables_at_once=5, sample_size=100):\n",
    "    for dataset in datasets:\n",
    "        cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "        tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "        tables_data = []\n",
    "        table_names = []\n",
    "        processed_count = 0\n",
    "\n",
    "        for table in tqdm(tables, desc=f\"Processing tables for dataset {dataset}...\"):\n",
    "            if table.endswith(\".csv\"):\n",
    "                df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "                table_name = table.split(\".csv\")[0]\n",
    "\n",
    "                # Get NE columns and correct QIDs from GT\n",
    "                ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "                \n",
    "                # Add table data and name to lists\n",
    "                tables_data.append(df)\n",
    "                table_names.append((dataset, table_name, df, ne_cols, correct_qids))\n",
    "                processed_count += 1\n",
    "\n",
    "                # If we reach the maximum number of tables, make the API call\n",
    "                if processed_count >= max_tables_at_once:\n",
    "                    # Call the NER API to classify columns for these tables (with sampling)\n",
    "                    ner_responses = classify_columns_with_api(tables_data, sample_size=sample_size)\n",
    "                    \n",
    "                    if ner_responses:\n",
    "                        for table_idx, table_response in enumerate(ner_responses):\n",
    "                            table_key = list(table_response.keys())[0]  # Extract table key (e.g., 'table_1', 'table_2')\n",
    "                            dataset, table_name, df, ne_cols, correct_qids = table_names[table_idx]\n",
    "                            # Onboard the data into MongoDB\n",
    "                            onboard_data(dataset, table_name, df, ne_cols, correct_qids, table_response[table_key])\n",
    "\n",
    "                    # Reset tables data for the next batch\n",
    "                    tables_data = []\n",
    "                    table_names = []\n",
    "                    processed_count = 0\n",
    "            \n",
    "        # In case there are remaining tables that haven't been processed yet\n",
    "        if tables_data:\n",
    "            ner_responses = classify_columns_with_api(tables_data, sample_size=sample_size)\n",
    "            if ner_responses:\n",
    "                for table_idx, table_response in enumerate(ner_responses):\n",
    "                    table_key = list(table_response.keys())[0]  # Extract table key\n",
    "                    dataset, table_name, df, ne_cols, correct_qids = table_names[table_idx]\n",
    "                    onboard_data(dataset, table_name, df, ne_cols, correct_qids, table_response[table_key])\n",
    "\n",
    "        # Initialize dataset-level trace after onboarding all tables in the dataset\n",
    "        dataset_trace_collection.update_one(\n",
    "            {\"dataset_name\": dataset},\n",
    "            {\"$setOnInsert\": {\n",
    "                \"total_tables\": len(tables),\n",
    "                \"processed_tables\": 0,\n",
    "                \"total_rows\": 0,  # Updated as tables are processed\n",
    "                \"processed_rows\": 0,\n",
    "                \"status\": \"PENDING\"\n",
    "            }},\n",
    "            upsert=True\n",
    "        ) \n",
    "\n",
    "# Example of running the function with batching and sampling\n",
    "process_tables(datasets, max_tables_at_once=10, sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install column-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables for dataset Round1_T2D...: 100%|██████████| 64/64 [00:04<00:00, 15.50it/s]\n",
      "Processing tables for dataset Round3_2019...: 100%|██████████| 2161/2161 [02:09<00:00, 16.69it/s]\n",
      "Processing tables for dataset 2T_2020...:  60%|██████    | 108/180 [07:30<07:06,  5.92s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from column_classifier import ColumnClassifier\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3_2019\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Initialize the column classifier\n",
    "classifier = ColumnClassifier(model_type='fast')\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    ne_cols = {int(row[3]): None for row in filtered_cea_gt.itertuples()}  # Column index as integer\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{int(row[1])-1}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to determine tag based on classification\n",
    "def determine_tag(classification):\n",
    "    ne_types = [\"LOCATION\", \"ORGANIZATION\", \"PERSON\", \"OTHER\"]\n",
    "    if classification in ne_types:\n",
    "        return \"NE\"\n",
    "    return \"LIT\"\n",
    "\n",
    "# Function to onboard data into MongoDB\n",
    "def onboard_data(dataset_name, table_name, df, ne_cols, lit_cols, correct_qids):\n",
    "    all_columns = {str(i) for i in range(len(df.columns))}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        document = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": index,\n",
    "            \"data\": row.to_dict(),\n",
    "            \"classified_columns\": {\n",
    "                \"NE\": ne_cols,\n",
    "                \"LIT\": lit_cols,\n",
    "                \"UNCLASSIFIED\": list(all_columns - (set(ne_cols.keys()) | set(lit_cols.keys())))\n",
    "            },\n",
    "            \"context_columns\": list(all_columns),\n",
    "            \"correct_qids\": correct_qids,\n",
    "            \"status\": \"TODO\"\n",
    "        }\n",
    "\n",
    "        input_collection.update_one(\n",
    "            {\"dataset_name\": dataset_name, \"table_name\": table_name, \"row_id\": index},\n",
    "            {\"$set\": document},\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "    table_trace_collection.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"total_rows\": len(df),\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    process_queue.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"status\": \"QUEUED\",\n",
    "            \"table_name\": table_name,\n",
    "            \"dataset_name\": dataset_name\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "# Main processing loop for onboarding datasets\n",
    "def process_tables(datasets, max_tables_at_once=5):\n",
    "    for dataset in datasets:\n",
    "        cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "        tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "        tables_data = []\n",
    "        table_names = []\n",
    "        processed_count = 0\n",
    "\n",
    "        for table in tqdm(tables, desc=f\"Processing tables for dataset {dataset}...\"):\n",
    "            if table.endswith(\".csv\"):\n",
    "                df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "                table_name = table.split(\".csv\")[0]\n",
    "\n",
    "                df_sampled = df.sample(n=min(100, len(df)), random_state=42)\n",
    "\n",
    "                ne_cols_gt, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "\n",
    "                tables_data.append(df_sampled)\n",
    "                table_names.append((dataset, table_name, df, ne_cols_gt, correct_qids))\n",
    "                processed_count += 1\n",
    "                if processed_count >= max_tables_at_once:\n",
    "                    ner_responses = classifier.classify_multiple_tables(tables_data)\n",
    "                    if ner_responses:\n",
    "                        for table_idx, table_response in enumerate(ner_responses):\n",
    "                            dataset, table_name, df, ne_cols_gt, correct_qids = table_names[table_idx]\n",
    "                            ne_cols_classified = {}\n",
    "                            lit_cols_classified = {}\n",
    "                            for table in table_response:\n",
    "                                ner_info_columns = table_response[table]\n",
    "                                for col_name in ner_info_columns:\n",
    "                                    ner_info = ner_info_columns[col_name]\n",
    "                                    classification = ner_info[\"classification\"]\n",
    "                                    tag = determine_tag(classification)\n",
    "                                \n",
    "                                    col_idx = df.columns.get_loc(col_name)  # Get the integer index of the column\n",
    "                                    col_idx_str = str(col_idx)\n",
    "                        \n",
    "                                    if col_idx in ne_cols_gt:\n",
    "                                        # If GT classifies as NE, we trust the GT and update with NER type\n",
    "                                        if tag == \"LIT\":\n",
    "                                            classification = \"OTHER\"\n",
    "                                        ne_cols_classified[col_idx_str] = classification\n",
    "                                    elif tag == \"LIT\":\n",
    "                                        lit_cols_classified[col_idx_str] = classification\n",
    "\n",
    "                            onboard_data(dataset, table_name, df, ne_cols_classified, lit_cols_classified, correct_qids)\n",
    "                    tables_data = []\n",
    "                    table_names = []\n",
    "                    processed_count = 0\n",
    "            \n",
    "        if tables_data:\n",
    "            ner_responses = classifier.classify_multiple_tables(tables_data)\n",
    "            if ner_responses:\n",
    "                for table_idx, table_response in enumerate(ner_responses):\n",
    "                    dataset, table_name, df, ne_cols_gt, correct_qids = table_names[table_idx]\n",
    "                    ne_cols_classified = {}\n",
    "                    lit_cols_classified = {}\n",
    "                    for table_name in table_response:\n",
    "                        ner_info_columns = table_response[table_name]\n",
    "                        for col_name in ner_info_columns:\n",
    "                            ner_info = ner_info_columns[col_name]\n",
    "                            classification = ner_info[\"classification\"]\n",
    "                            tag = determine_tag(classification)\n",
    "    \n",
    "                            col_idx = df.columns.get_loc(col_name)  # Get the integer index of the column\n",
    "                            col_idx_str = str(col_idx)\n",
    "                            \n",
    "                            if col_idx in ne_cols_gt:\n",
    "                                # If GT classifies as NE, we trust the GT and update with NER type\n",
    "                                if tag == \"LIT\":\n",
    "                                    classification = \"OTHER\"\n",
    "                                ne_cols_classified[col_idx_str] = classification\n",
    "                            elif tag == \"LIT\":\n",
    "                                lit_cols_classified[col_idx_str] = classification\n",
    "    \n",
    "                    onboard_data(dataset, table_name, df, ne_cols_classified, lit_cols_classified, correct_qids)\n",
    "\n",
    "        dataset_trace_collection.update_one(\n",
    "            {\"dataset_name\": dataset},\n",
    "            {\"$setOnInsert\": {\n",
    "                \"total_tables\": len(tables),\n",
    "                \"processed_tables\": 0,\n",
    "                \"total_rows\": 0,\n",
    "                \"processed_rows\": 0,\n",
    "                \"status\": \"PENDING\"\n",
    "            }},\n",
    "            upsert=True\n",
    "        ) \n",
    "\n",
    "# Example of running the function with batching\n",
    "process_tables(datasets, max_tables_at_once=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_cea_gt = cea_gt[cea_gt[0] == \"58891288_0_1117541047012405958\"]\n",
    "ne_cols = {int(row[3]): None for row in filtered_cea_gt.itertuples()}  \n",
    "ne_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=33, _1='58891288_0_1117541047012405958', _2=95, _3=1, _4='https://www.wikidata.org/entity/Q42198')\n"
     ]
    }
   ],
   "source": [
    "for row in filtered_cea_gt.itertuples():\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_gt = pd.read_csv(f\"./Datasets/Round1_T2D/gt/cea.csv\", header=None)\n",
    "get_ne_cols_and_correct_qids(\"58891288_0_1117541047012405958\", cea_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50245608_0_871275842592178099</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.wikidata.org/entity/Q194413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22864497_0_8632623712684511496</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.wikidata.org/entity/Q219795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66009064_0_9148652238372261251</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.wikidata.org/entity/Q558664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21362676_0_6854186738074119688</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.wikidata.org/entity/Q309048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50245608_0_871275842592178099</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.wikidata.org/entity/Q820753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>58891288_0_1117541047012405958</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.wikidata.org/entity/Q42047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>90196673_0_5458330029110291950</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.wikidata.org/entity/Q199203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>58891288_0_1117541047012405958</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.wikidata.org/entity/Q135465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8077</th>\n",
       "      <td>53822652_0_5767892317858575530</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.wikidata.org/entity/Q200396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8078</th>\n",
       "      <td>22864497_0_8632623712684511496</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.wikidata.org/entity/Q6738126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8079 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0    1  2  \\\n",
       "0      50245608_0_871275842592178099  154  0   \n",
       "1     22864497_0_8632623712684511496  227  0   \n",
       "2     66009064_0_9148652238372261251   15  0   \n",
       "3     21362676_0_6854186738074119688   75  1   \n",
       "4      50245608_0_871275842592178099  186  0   \n",
       "...                              ...  ... ..   \n",
       "8074  58891288_0_1117541047012405958   18  1   \n",
       "8075  90196673_0_5458330029110291950  382  0   \n",
       "8076  58891288_0_1117541047012405958   51  1   \n",
       "8077  53822652_0_5767892317858575530  117  1   \n",
       "8078  22864497_0_8632623712684511496  208  0   \n",
       "\n",
       "                                             3  \n",
       "0      https://www.wikidata.org/entity/Q194413  \n",
       "1      https://www.wikidata.org/entity/Q219795  \n",
       "2      https://www.wikidata.org/entity/Q558664  \n",
       "3      https://www.wikidata.org/entity/Q309048  \n",
       "4      https://www.wikidata.org/entity/Q820753  \n",
       "...                                        ...  \n",
       "8074    https://www.wikidata.org/entity/Q42047  \n",
       "8075   https://www.wikidata.org/entity/Q199203  \n",
       "8076   https://www.wikidata.org/entity/Q135465  \n",
       "8077   https://www.wikidata.org/entity/Q200396  \n",
       "8078  https://www.wikidata.org/entity/Q6738126  \n",
       "\n",
       "[8079 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cea_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rank': {'classification': 'NUMBER', 'probabilities': {'NUMBER': 1.0}}, 'Title': {'classification': 'OTHER', 'probabilities': {'LOCATION': 0.14, 'ORGANIZATION': 0.16, 'PERSON': 0.32, 'OTHER': 0.22, 'STRING': 0.08}}, 'Year': {'classification': 'NUMBER', 'probabilities': {'NUMBER': 1.0, 'DATE': 0.88}}, 'Director(s)': {'classification': 'PERSON', 'probabilities': {'PERSON': 1.0}}, 'Overall Rank': {'classification': 'NUMBER', 'probabilities': {'NUMBER': 1.0}}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'table_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m test:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'table_1'"
     ]
    }
   ],
   "source": [
    "for table in test:\n",
    "    print(table[\"table_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables for dataset Round1_T2D...: 100%|██████████| 64/64 [00:02<00:00, 25.22it/s]\n",
      "Processing tables for dataset Round3_2019...: 100%|██████████| 2161/2161 [01:48<00:00, 19.90it/s]\n",
      "Processing tables for dataset 2T_2020...: 100%|██████████| 180/180 [13:25<00:00,  4.47s/it]\n",
      "Processing tables for dataset Round4_2020...: 100%|██████████| 22207/22207 [23:07<00:00, 16.01it/s]   \n",
      "Processing tables for dataset HardTablesR2...: 100%|██████████| 1751/1751 [00:23<00:00, 74.15it/s]\n",
      "Processing tables for dataset HardTablesR3...: 100%|██████████| 7207/7207 [00:52<00:00, 138.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from column_classifier import ColumnClassifier\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "def ensure_indexes():\n",
    "    input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "    table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "ensure_indexes()\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3_2019\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Initialize the column classifier\n",
    "classifier = ColumnClassifier(model_type='fast')\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    ne_cols = {int(row[3]): None for row in filtered_cea_gt.itertuples()}  # Column index as integer\n",
    "    correct_qids = {f\"{int(row[1])-1}-{row[2]}\": row[3].split(\"/\")[-1] for _, row in filtered_cea_gt.iterrows()}\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to determine tag based on classification\n",
    "def determine_tag(classification):\n",
    "    return \"NE\" if classification in [\"LOCATION\", \"ORGANIZATION\", \"PERSON\", \"OTHER\"] else \"LIT\"\n",
    "\n",
    "# Function to batch onboard data into MongoDB\n",
    "def onboard_data_batch(dataset_name, table_name, df, ne_cols, lit_cols, correct_qids):\n",
    "    all_columns = set([str(i) for i in range(len(df.columns))])\n",
    "    classified_ne_columns = set(ne_cols.keys())\n",
    "    classified_lit_columns = set(lit_cols.keys())\n",
    "    unclassified_columns = all_columns - (classified_ne_columns | classified_lit_columns)\n",
    "\n",
    "    documents = []\n",
    "    for index, row in df.iterrows():\n",
    "        documents.append({\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": index,\n",
    "            \"data\": row.to_dict(),\n",
    "            \"classified_columns\": {\n",
    "                \"NE\": ne_cols,\n",
    "                \"LIT\": lit_cols,\n",
    "                \"UNCLASSIFIED\": list(unclassified_columns)\n",
    "            },\n",
    "            \"context_columns\": list(all_columns),\n",
    "            \"correct_qids\": correct_qids,\n",
    "            \"status\": \"TODO\"\n",
    "        })\n",
    "    \n",
    "    if documents:\n",
    "        input_collection.insert_many(documents)  # Batch insert the documents\n",
    "\n",
    "    # Update trace collections in MongoDB\n",
    "    table_trace_collection.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"total_rows\": len(df),\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    process_queue.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"status\": \"QUEUED\",\n",
    "            \"table_name\": table_name,\n",
    "            \"dataset_name\": dataset_name\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "# Main processing loop for onboarding datasets\n",
    "def process_tables(datasets, max_tables_at_once=5):\n",
    "    for dataset in datasets:\n",
    "        cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "        tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "        batch_tables_data = []\n",
    "        batch_table_names = []\n",
    "\n",
    "        for table in tqdm(tables, desc=f\"Processing tables for dataset {dataset}...\"):\n",
    "            if table.endswith(\".csv\"):\n",
    "                df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "                table_name = table.split(\".csv\")[0]\n",
    "\n",
    "                df_sampled = df.sample(n=min(100, len(df)), random_state=42)\n",
    "\n",
    "                # Get NE columns and correct QIDs from GT\n",
    "                ne_cols_gt, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "\n",
    "                batch_tables_data.append(df_sampled)\n",
    "                batch_table_names.append((dataset, table_name, df, ne_cols_gt, correct_qids))\n",
    "\n",
    "                if len(batch_tables_data) >= max_tables_at_once:\n",
    "                    process_table_batch(batch_tables_data, batch_table_names)\n",
    "                    batch_tables_data = []\n",
    "                    batch_table_names = []\n",
    "\n",
    "        # Process remaining tables in the batch\n",
    "        if batch_tables_data:\n",
    "            process_table_batch(batch_tables_data, batch_table_names)\n",
    "\n",
    "        # Initialize dataset-level trace after processing all tables\n",
    "        dataset_trace_collection.update_one(\n",
    "            {\"dataset_name\": dataset},\n",
    "            {\"$setOnInsert\": {\n",
    "                \"total_tables\": len(tables),\n",
    "                \"processed_tables\": 0,\n",
    "                \"total_rows\": 0,\n",
    "                \"processed_rows\": 0,\n",
    "                \"status\": \"PENDING\"\n",
    "            }},\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "# Process a batch of tables\n",
    "def process_table_batch(batch_tables_data, batch_table_names):\n",
    "    ner_responses = classifier.classify_multiple_tables(batch_tables_data)\n",
    "\n",
    "    if ner_responses:\n",
    "        for table_idx, table_response in enumerate(ner_responses):\n",
    "            dataset, table_name, df, ne_cols_gt, correct_qids = batch_table_names[table_idx]\n",
    "            ne_cols_classified = {}\n",
    "            lit_cols_classified = {}\n",
    "\n",
    "            # Process each table's response\n",
    "            table_key = list(table_response.keys())[0]\n",
    "            ner_info_columns = table_response[table_key]\n",
    "\n",
    "            for col_name, ner_info in ner_info_columns.items():\n",
    "                classification = ner_info[\"classification\"]\n",
    "                tag = determine_tag(classification)\n",
    "\n",
    "                col_idx = df.columns.get_loc(col_name)  # Get the integer index of the column\n",
    "                col_idx_str = str(col_idx)\n",
    "\n",
    "                if col_idx in ne_cols_gt:\n",
    "                    # Trust GT for NE columns but use NER to update\n",
    "                    if tag == \"LIT\":\n",
    "                        classification = \"OTHER\"\n",
    "                    ne_cols_classified[col_idx_str] = classification\n",
    "                elif tag == \"LIT\":\n",
    "                    lit_cols_classified[col_idx_str] = classification\n",
    "\n",
    "            onboard_data_batch(dataset, table_name, df, ne_cols_classified, lit_cols_classified, correct_qids)\n",
    "\n",
    "# Example of running the function with batching\n",
    "process_tables(datasets, max_tables_at_once=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from column_classifier import ColumnClassifier\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "def ensure_indexes():\n",
    "    input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "    table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "ensure_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from column_classifier import ColumnClassifier\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"training_data\"]\n",
    "doc = input_collection.find_one({})\n",
    "model = load_model(\"trained_models/neural_ranker.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "def extract_features(candidate):\n",
    "    \"\"\"Extract features for the ML model from a candidate.\"\"\"\n",
    "    return [candidate['features'].get(feature, 0) for feature in candidate[\"features\"]]\n",
    "\n",
    "candidates = doc[\"candidates\"]['0']\n",
    "candidate_features = [extract_features(candidate) for candidate in candidates]\n",
    "# Convert the features list to a NumPy array\n",
    "candidate_features_array = np.array(candidate_features)\n",
    "\n",
    "# Use the model to predict on the array\n",
    "predictions = model.predict(candidate_features_array)\n",
    "# Use the probability of the positive class as the score\n",
    "positive_class_scores = predictions[:, 1]  # Assuming index 1 is the positive class\n",
    "\n",
    "# Assign the scores to candidates\n",
    "for candidate, score in zip(candidates, positive_class_scores):\n",
    "    candidate['score'] = round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17935325, 0.8206468 ],\n",
       "       [0.8480819 , 0.15191813],\n",
       "       [0.8018286 , 0.19817129],\n",
       "       [0.66995764, 0.3300423 ],\n",
       "       [0.8378532 , 0.16214679],\n",
       "       [0.8378532 , 0.16214679],\n",
       "       [0.93741906, 0.06258096],\n",
       "       [0.8928639 , 0.10713608],\n",
       "       [0.96457493, 0.03542513],\n",
       "       [0.97371554, 0.02628445]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8206468 , 0.15191813, 0.19817129, 0.3300423 , 0.16214679,\n",
       "       0.16214679, 0.06258096, 0.10713608, 0.03542513, 0.02628445],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Q19355',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '1997 film directed by Roberto Benigni',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.09,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0857,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.3,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['drama',\n",
       "   'la',\n",
       "   'è',\n",
       "   'benigni',\n",
       "   'comedy',\n",
       "   '116',\n",
       "   'roberto',\n",
       "   'bella',\n",
       "   'vita',\n",
       "   'holocaust'],\n",
       "  'score': 0.82},\n",
       " {'id': 'Q6544851',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': 'soundtrack album to the 1997 film',\n",
       "  'types': [{'id': 'Q482994', 'name': 'album'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0278,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.15},\n",
       " {'id': 'Q3825014',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '1943 film by Carlo Ludovico Bragaglia',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.15,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['la', 'è', 'comedy', 'bella', 'vita'],\n",
       "  'score': 0.2},\n",
       " {'id': 'Q2250632',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '1979 film by Grigori Chukhrai',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.02,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.15,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['drama', 'la', 'è', 'bella', 'vita'],\n",
       "  'score': 0.33},\n",
       " {'id': 'Q17150423',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': 'song',\n",
       "  'types': [{'id': 'Q105543609', 'name': 'musical work/composition'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.16},\n",
       " {'id': 'Q21281489',\n",
       "  'name': 'La Vita è Bella',\n",
       "  'description': \"Claire Kuo's soundtrack album\",\n",
       "  'types': [{'id': 'Q482994', 'name': 'album'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.16},\n",
       " {'id': 'Q16774760',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': 'Wikimedia disambiguation page',\n",
       "  'types': [{'id': 'Q4167410', 'name': 'Wikimedia disambiguation page'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.01,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 3,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.06},\n",
       " {'id': 'Q3825016',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '',\n",
       "  'types': [{'id': 'Q7725634', 'name': 'literary work'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.06,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['roberto', 'benigni'],\n",
       "  'score': 0.11},\n",
       " {'id': 'Q16084568',\n",
       "  'name': 'E... la vita è bella',\n",
       "  'description': '1985 film by Boro Drašković',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 5,\n",
       "   'length_entity': 20,\n",
       "   'popularity': 0.01,\n",
       "   'ed_score': 0.75,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.03,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['drama'],\n",
       "  'score': 0.04},\n",
       " {'id': 'Q1381883',\n",
       "  'name': 'Bella è la vita',\n",
       "  'description': 'French television soap opera',\n",
       "  'types': [{'id': 'Q5398426', 'name': 'television series'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.01,\n",
       "   'ed_score': 0.2,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.03,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['la'],\n",
       "  'score': 0.03}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Q19355',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '1997 film directed by Roberto Benigni',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.09,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0857,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.3,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['drama',\n",
       "   'la',\n",
       "   'è',\n",
       "   'benigni',\n",
       "   'comedy',\n",
       "   '116',\n",
       "   'roberto',\n",
       "   'bella',\n",
       "   'vita',\n",
       "   'holocaust'],\n",
       "  'score': 0.35},\n",
       " {'id': 'Q6544851',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': 'soundtrack album to the 1997 film',\n",
       "  'types': [{'id': 'Q482994', 'name': 'album'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0278,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.29},\n",
       " {'id': 'Q3825014',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '1943 film by Carlo Ludovico Bragaglia',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.15,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['la', 'è', 'comedy', 'bella', 'vita'],\n",
       "  'score': 0.29},\n",
       " {'id': 'Q2250632',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '1979 film by Grigori Chukhrai',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.02,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.15,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['drama', 'la', 'è', 'bella', 'vita'],\n",
       "  'score': 0.29},\n",
       " {'id': 'Q17150423',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': 'song',\n",
       "  'types': [{'id': 'Q105543609', 'name': 'musical work/composition'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.28},\n",
       " {'id': 'Q21281489',\n",
       "  'name': 'La Vita è Bella',\n",
       "  'description': \"Claire Kuo's soundtrack album\",\n",
       "  'types': [{'id': 'Q482994', 'name': 'album'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.28},\n",
       " {'id': 'Q16774760',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': 'Wikimedia disambiguation page',\n",
       "  'types': [{'id': 'Q4167410', 'name': 'Wikimedia disambiguation page'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.01,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.12,\n",
       "   'kind': 3,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['bella', 'la', 'vita', 'è'],\n",
       "  'score': 0.28},\n",
       " {'id': 'Q3825016',\n",
       "  'name': 'La vita è bella',\n",
       "  'description': '',\n",
       "  'types': [{'id': 'Q7725634', 'name': 'literary work'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.06,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['roberto', 'benigni'],\n",
       "  'score': 0.27},\n",
       " {'id': 'Q16084568',\n",
       "  'name': 'E... la vita è bella',\n",
       "  'description': '1985 film by Boro Drašković',\n",
       "  'types': [{'id': 'Q11424', 'name': 'film'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 5,\n",
       "   'length_entity': 20,\n",
       "   'popularity': 0.01,\n",
       "   'ed_score': 0.75,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.03,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['drama'],\n",
       "  'score': 0.2},\n",
       " {'id': 'Q1381883',\n",
       "  'name': 'Bella è la vita',\n",
       "  'description': 'French television soap opera',\n",
       "  'types': [{'id': 'Q5398426', 'name': 'television series'}],\n",
       "  'features': {'ntoken_mention': 4,\n",
       "   'length_mention': 15,\n",
       "   'ntoken_entity': 4,\n",
       "   'length_entity': 15,\n",
       "   'popularity': 0.01,\n",
       "   'ed_score': 0.2,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.03,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': 4},\n",
       "  'matched_words': ['la'],\n",
       "  'score': 0.06}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
